#敏感词检查的程序制作
##前期思路
使用python完成程序。

首先用jieba将文章分词，在视图遍历所得分词，并与词典内词语相对比，如果匹配则用[-1s]替换。

##了解并测试jieba
首先pip install jieba,安装jieba,然后开始进行测试。

一开始我并没有了解jieba原理，认为jieba储存方式是一个元组或list，于是print，测试了很多都会得到各种错误，后来在差错中发现```seg_list```是一个生成器，解决了很多问题。值得注意的是py3中next变为```__next__```。

嗯接下来是对python迭代器的再学习和对生成器的学习。

了解到生成器只能迭代一次，也即不储存，很麻烦，于是决定将生成器生成的数据转换到list里。这里了解了各种生成list的方法并进行尝试，发现各种方法基本都可以使用（for循环，append等），于是选择了最快的list().

至此jieba的测试和初步使用完毕。

##尝试替换敏感词

读取文件后开始遍历之前完成的分词。用for进行循环并判断，如果分词与敏感词重合就进行替换，最后写入outputs.txt。整体思路大概就是这样。

###注意点
1. windows版的python对文件编码较为严格，所以要转换编码方可读取写入文件。
2. 不知道什么原因绝对路径可以成功但用linux模式的相对路径不行。```./test/inputs.txt```会报错。所以我引入os和sys定义了相对路径方便使用。

#结尾
##问题与不足
1. 开头莫名有一个不知是什么的被替换为[-1s]。
2. 原文极难还原。其中的换行和空格等很难还原，不知道是不是我的思路有问题。
3. jieba分词导致一些长词的敏感词不能被识别，网址不能被识别，经测试（jieba1.py)，是因为jieba分词将它们长词和网址中间分开。

目前就是这样，希望学长（姐）多多指点，感激不尽。
10.10

##更新 
###10.12
3. 关于问题3，想到先载入敏感词，再进行分词，果然问题解决。
1. 再重新尝试后发现问题一消失，无法复现。
2. 问题二仍然存在，试图加入^p作为新词失败。

###10.14
问题二暂时解决。想法是先将文件中所有的\n转换成其他字符串，此字符串不在敏感词中，分词完毕后再转换回来。

遇到了几个问题，并一一寻找解决方法：

1. 一开始直接想在循环中插入判断是否为\n的语句，后来发现在jieba中已经默认将\n去除，甚至加入字典也不行。
2. 在读取文件，写入的过程中遇到错误，直接在原文件内修改暂时无法成功，只能退而选择两个文件。
3. 发现\n在utf中不能显示，只能进行格式转换。传统decode，encode并不能奏效，最后选择在读取时就进行选择。后来又发现jieba只支持utf（理所当然），虽然解决办法很简单，但花了很长时间才想到如何解决。。
4. 本地txt打开和notepad++打开，文字的分行符不同，是因为本地自动根据长度会进行分行，目前只能用notepad++等工具进行分行操作，如果直接用notepad结果会多分行。

最重要的是感觉过程变复杂了，因为替换，速度下降。应该是没有想到更好的解决方法，请学长学姐多多指点。

接下来的学习主要围绕10.14第二点和第四点的更优的解决办法。如果什么时候突然想到更好的解决10.12问题二的解决办法再从新来。